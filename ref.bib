@inproceedings{hjz22,
  author       = {Nika Haghtalab and
                  Michael I. Jordan and
                  Eric Zhao},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {On-Demand Sampling: Learning Optimally from Multiple Distributions},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/02917acec264a52a729b99d9bc857909-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:31 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/HaghtalabJ022.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ahz23,
  author       = {Pranjal Awasthi and
                  Nika Haghtalab and
                  Eric Zhao},
  editor       = {Gergely Neu and
                  Lorenzo Rosasco},
  title        = {Open Problem: The Sample Complexity of Multi-Distribution Learning
                  for {VC} Classes},
  booktitle    = {The Thirty Sixth Annual Conference on Learning Theory, {COLT} 2023,
                  12-15 July 2023, Bangalore, India},
  series       = {Proceedings of Machine Learning Research},
  volume       = {195},
  pages        = {5943--5949},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v195/awasthi23a.html},
  timestamp    = {Wed, 06 Sep 2023 17:49:05 +0200},
  biburl       = {https://dblp.org/rec/conf/colt/AwasthiH023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zzc+24,
  author       = {Zihan Zhang and
                  Wenhao Zhan and
                  Yuxin Chen and
                  Simon S. Du and
                  Jason D. Lee},
  editor       = {Shipra Agrawal and
                  Aaron Roth},
  title        = {Optimal Multi-Distribution Learning},
  booktitle    = {The Thirty Seventh Annual Conference on Learning Theory, June 30 -
                  July 3, 2023, Edmonton, Canada},
  series       = {Proceedings of Machine Learning Research},
  volume       = {247},
  pages        = {5220--5223},
  publisher    = {{PMLR}},
  year         = {2024},
  url          = {https://proceedings.mlr.press/v247/zhang24b.html},
  timestamp    = {Sun, 04 Aug 2024 16:27:44 +0200},
  biburl       = {https://dblp.org/rec/conf/colt/ZhangZCDL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{sfboosting,
    author = {Schapire, Robert E. and Freund, Yoav},
    title = {Boosting: Foundations and Algorithms},
    publisher = {The MIT Press},
    year = {2012},
    month = {05},
    abstract = {An accessible introduction and essential reference for an approach to machine learning that creates highly accurate prediction rules by combining many weak and inaccurate ones.Boosting is an approach to machine learning based on the idea of creating a highly accurate predictor by combining many weak and inaccurate “rules of thumb.” A remarkably rich theory has evolved around boosting, with connections to a range of topics, including statistics, game theory, convex optimization, and information geometry. Boosting algorithms have also enjoyed practical success in such fields as biology, vision, and speech processing. At various times in its history, boosting has been perceived as mysterious, controversial, even paradoxical.This book, written by the inventors of the method, brings together, organizes, simplifies, and substantially extends two decades of research on boosting, presenting both theory and applications in a way that is accessible to readers from diverse backgrounds while also providing an authoritative reference for advanced researchers. With its introductory treatment of all material and its inclusion of exercises in every chapter, the book is appropriate for course use as well.The book begins with a general introduction to machine learning algorithms and their analysis; then explores the core theory of boosting, especially its ability to generalize; examines some of the myriad other theoretical viewpoints that help to explain and understand boosting; provides practical extensions of boosting for more complex learning problems; and finally presents a number of advanced theoretical topics. Numerous applications and practical illustrations are offered throughout.},
    isbn = {9780262301183},
    doi = {10.7551/mitpress/8291.001.0001},
    url = {https://doi.org/10.7551/mitpress/8291.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2280056/book\_9780262301183.pdf},
}